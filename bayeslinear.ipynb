{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pwd",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\nnp.random.seed(42)\n\n \n# Matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport matplotlib\nmatplotlib.rcParams['font.size'] = 16\nmatplotlib.rcParams['figure.figsize'] = (9, 9)\n\nimport seaborn as sns\n\nfrom IPython.core.pylabtools import figsize\n\n# Scipy helper functions\nfrom scipy.stats import percentileofscore\nfrom scipy import stats",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Standard ML Models for comparison\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\n\n# Splitting data into training/testing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n\n# Distributions\nimport scipy",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# PyMC3 for Bayesian Inference\nimport pymc3 as pm",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Read in class scores\ndf = pd.read_csv('/home/nbuser/library/student-mat.csv')\n\n# Filter out grades that were 0\ndf = df[~df['G3'].isin([0, 1])]\n\ndf = df.rename(columns={'G3': 'Grade'})\n\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\ndata = pd.read_csv('/home/nbuser/library/student-por.csv', delimiter=';')\ndata.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Describe for Numerical Columns"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Value Counts for Categorical Columns"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Print the value counts for categorical columns\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        print('\\nColumn Name:', col,)\n        print(df[col].value_counts())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Distribution of Grades"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df['Grade'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df['Grade'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Bar plot of grades\nplt.bar(df['Grade'].value_counts().index, \n        df['Grade'].value_counts().values,\n         fill = 'navy', edgecolor = 'k', width = 1)\nplt.xlabel('Grade'); plt.ylabel('Count'); plt.title('Distribution of Final Grades');\nplt.xticks(list(range(5, 20)));",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Grade Distribution by Different Categorical Variables"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Grade distribution by address\nsns.kdeplot(df.ix[df['address'] == 'U', 'Grade'], label = 'Urban', shade = True)\nsns.kdeplot(df.ix[df['address'] == 'R', 'Grade'], label = 'Rural', shade = True)\nplt.xlabel('Grade'); plt.ylabel('Density'); plt.title('Density Plot of Final Grades by Location');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Grade distribution by Guardian\nsns.kdeplot(df.ix[df['guardian'] == 'father', 'Grade'], label = 'Father', shade = True)\nsns.kdeplot(df.ix[df['guardian'] == 'mother', 'Grade'], label = 'Mother', shade = True)\nsns.kdeplot(df.ix[df['guardian'] == 'other', 'Grade'], label = 'Other', shade = True)\nplt.xlabel('Grade'); plt.ylabel('Density'); plt.title('Density Plot of Final Grades by Guardian');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Grade distribution by internet\nsns.kdeplot(df.ix[df['internet'] == 'yes', 'Grade'], label = 'Internet', shade = True)\nsns.kdeplot(df.ix[df['internet'] == 'no', 'Grade'], label = 'No Internet', shade = True)\nplt.xlabel('Grade'); plt.ylabel('Density'); plt.title('Density Plot of Final Grades by Internet Access');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Grade distribution by school\nsns.kdeplot(df.ix[df['school'] == 'GP', 'Grade'], label = 'GP', shade = True)\nsns.kdeplot(df.ix[df['school'] == 'MS', 'Grade'], label = 'MS', shade = True)\nplt.xlabel('Grade'); plt.ylabel('Count'); plt.title('Distribution of Final Grades by School');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Look at distribution of schools by address\nschools = df.groupby(['school'])['address'].value_counts()\nschools",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Grade Percentiles"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Calculate percentile for grades\ndf['percentile'] = df['Grade'].apply(lambda x: percentileofscore(df['Grade'], x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Plot percentiles for grades\nplt.figure(figsize = (8, 6))\nplt.plot(df['Grade'], df['percentile'], 'o')\nplt.xticks(range(0, 20, 2), range(0, 20, 2))\nplt.xlabel('Score'); plt.ylabel('Percentile'); plt.title('Grade Percentiles');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print('50th percentile score:', np.min(df.loc[df['percentile'] > 50, 'Grade']))\nprint('Minimum Score needed for 90th percentile:', np.min(df.loc[df['percentile'] > 90, 'Grade']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Variable Correlations with Final Grade\nNumerical Correlations"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Correlations of numerical values\ndf.corr()['Grade'].sort_values()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Categorical Correlations using One-Hot Encoding"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Select only categorical variables\ncategory_df = df.select_dtypes('object')\n# One hot encode the variables\ndummy_df = pd.get_dummies(category_df)\n# Put the grade back in the dataframe\ndummy_df['Grade'] = df['Grade']\ndummy_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Correlations in one-hot encoded dataframe\ndummy_df.corr()['Grade'].sort_values()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Select 6 Most Correlated Variables with Final Score"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Takes in a dataframe, finds the most correlated variables with the\n# grade and returns training and testing datasets\ndef format_data(df):\n    # Targets are final grade of student\n    labels = df['Grade']\n    \n    # Drop the school and the grades from features\n    df = df.drop(columns=['school', 'G1', 'G2', 'percentile'])\n    \n    # One-Hot Encoding of Categorical Variables\n    df = pd.get_dummies(df)\n    \n    # Find correlations with the Grade\n    most_correlated = df.corr().abs()['Grade'].sort_values(ascending=False)\n    \n    # Maintain the top 6 most correlation features with Grade\n    most_correlated = most_correlated[:8]\n    \n    df = df.ix[:, most_correlated.index]\n    df = df.drop(columns = 'higher_no')\n    \n    # Split into training/testing sets with 25% split\n    X_train, X_test, y_train, y_test = train_test_split(df, labels, \n                                                        test_size = 0.25,\n                                                        random_state=42)\n    \n    return X_train, X_test, y_train, y_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = format_data(df)\nX_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Rename Variables"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Rename variables in train and teste\nX_train = X_train.rename(columns={'higher_yes': 'higher_edu', \n                                  'Medu': 'mother_edu',\n                                  'Fedu': 'father_edu'})\n\nX_test = X_test.rename(columns={'higher_yes': 'higher_edu', \n                                  'Medu': 'mother_edu',\n                                  'Fedu': 'father_edu'})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(X_train.shape)\nprint(X_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Pairs Plot of Selected Variables"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Calculate correlation coefficient\ndef corrfunc(x, y, **kws):\n    r, _ = stats.pearsonr(x, y)\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.1, .6), xycoords=ax.transAxes,\n               size = 24)\n    \ncmap = sns.cubehelix_palette(light=1, dark = 0.1,\n                             hue = 0.5, as_cmap=True)\n\nsns.set_context(font_scale=2)\n\n# Pair grid set up\ng = sns.PairGrid(X_train)\n\n# Scatter plot on the upper triangle\ng.map_upper(plt.scatter, s=10, color = 'red')\n\n# Distribution on the diagonal\ng.map_diag(sns.distplot, kde=False, color = 'red')\n\n# Density Plot and Correlation coefficients on the lower triangle\ng.map_lower(sns.kdeplot, cmap = cmap)\ng.map_lower(corrfunc);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create relation to the median grade column\nX_plot = X_train.copy()\nX_plot['relation_median'] = (X_plot['Grade'] >= 12)\nX_plot['relation_median'] = X_plot['relation_median'].replace({True: 'above', False: 'below'})\nX_plot = X_plot.drop(columns='Grade')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Selected Variables Distribution by Relation to Median\n\nplt.figure(figsize=(12, 12))\n# Plot the distribution of each variable colored\n# by the relation to the median grade\nfor i, col in enumerate(X_plot.columns[:-1]):\n    plt.subplot(3, 2, i + 1)\n    subset_above = X_plot[X_plot['relation_median'] == 'above']\n    subset_below = X_plot[X_plot['relation_median'] == 'below']\n    sns.kdeplot(subset_above[col], label = 'Above Median', color = 'green')\n    sns.kdeplot(subset_below[col], label = 'Below Median', color = 'red')\n    plt.legend(); plt.title('Distribution of %s' % col)\n    \nplt.tight_layout()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Establish Benchmarks\nMetrics\nFor this regression task, we will use two standard metrics:\nMean Absolute Error (MAE): Average of the absolute value of the difference between predictions and the true values\nRoot Mean Squared Error (RMSE): The square root of the average of the squared differences between the predictions and the true values.\nThe mean absolute error is more interpretable, but the root mean squared error penalizes larger errors more heavily. Either one may be appropriate depending on the situation."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Calculate mae and rmse\ndef evaluate_predictions(predictions, true):\n    mae = np.mean(abs(predictions - true))\n    rmse = np.sqrt(np.mean((predictions - true) ** 2))\n    \n    return mae, rmse",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Naive Baseline\nFor a regression task, a simple naive baseline is to guess the median value on the training set for all testing cases. If our machine learning model cannot better this simple baseline, then perhaps we should try a different approach!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Naive baseline is the median\nmedian_pred = X_train['Grade'].median()\nmedian_preds = [median_pred for _ in range(len(X_test))]\ntrue = X_test['Grade']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Display the naive baseline metrics\nmb_mae, mb_rmse = evaluate_predictions(median_preds, true)\nprint('Median Baseline  MAE: {:.4f}'.format(mb_mae))\nprint('Median Baseline RMSE: {:.4f}'.format(mb_rmse))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Standard Machine Learning Models\n# Evaluate several ml models by training on training set and testing on testing set\ndef evaluate(X_train, X_test, y_train, y_test):\n    # Names of models\n    model_name_list = ['Linear Regression', 'ElasticNet Regression',\n                      'Random Forest', 'Extra Trees', 'SVM',\n                       'Gradient Boosted', 'Baseline']\n    X_train = X_train.drop(columns='Grade')\n    X_test = X_test.drop(columns='Grade')\n    \n    # Instantiate the models\n    model1 = LinearRegression()\n    model2 = ElasticNet(alpha=1.0, l1_ratio=0.5)\n    model3 = RandomForestRegressor(n_estimators=50)\n    model4 = ExtraTreesRegressor(n_estimators=50)\n    model5 = SVR(kernel='rbf', degree=3, C=1.0, gamma='auto')\n    model6 = GradientBoostingRegressor(n_estimators=20)\n    \n    # Dataframe for results\n    results = pd.DataFrame(columns=['mae', 'rmse'], index = model_name_list)\n    \n    # Train and predict with each model\n    for i, model in enumerate([model1, model2, model3, model4, model5, model6]):\n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n        \n        # Metrics\n        mae = np.mean(abs(predictions - y_test))\n        rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n        \n        # Insert results into the dataframe\n        model_name = model_name_list[i]\n        results.ix[model_name, :] = [mae, rmse]\n    \n    # Median Value Baseline Metrics\n    baseline = np.median(y_train)\n    baseline_mae = np.mean(abs(baseline - y_test))\n    baseline_rmse = np.sqrt(np.mean((baseline - y_test) ** 2))\n    \n    results.ix['Baseline', :] = [baseline_mae, baseline_rmse]\n    \n    return results",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "results = evaluate(X_train, X_test, y_train, y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Visual Comparison of Models"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "figsize(12, 8)\nmatplotlib.rcParams['font.size'] = 16\n# Root mean squared error\nax =  plt.subplot(1, 2, 1)\nresults.sort_values('mae', ascending = True).plot.bar(y = 'mae', color = 'b', ax = ax)\nplt.title('Model Mean Absolute Error'); plt.ylabel('MAE');\n\n# Median absolute percentage error\nax = plt.subplot(1, 2, 2)\nresults.sort_values('rmse', ascending = True).plot.bar(y = 'rmse', color = 'r', ax = ax)\nplt.title('Model Root Mean Squared Error'); plt.ylabel('RMSE');\n\nplt.tight_layout()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "results",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print('The Gradient Boosted regressor is {:0.2f}% better than the baseline.'.format(\n    (100 * abs(results.loc['Gradient Boosted', 'mae'] - results.loc['Baseline', 'mae'])) / results.loc['Baseline', 'mae']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Formula from Ordinary Least Squares Linear Regression"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lr = LinearRegression()\nlr.fit(X_train.drop(columns='Grade'), y_train)\n\nols_formula = 'Grade = %0.2f +' % lr.intercept_\nfor i, col in enumerate(X_train.columns[1:]):\n    ols_formula += ' %0.2f * %s +' % (lr.coef_[i], col)\n    \n' '.join(ols_formula.split(' ')[:-1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Implementing Bayesian Linear Regression"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Formula for Bayesian Linear Regression (follows R formula syntax\nformula = 'Grade ~ ' + ' + '.join(['%s' % variable for variable in X_train.columns[1:]])\nformula",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Create Model in PyMC3 and Sample from Posterior\nWe now build the model using the formula defined above and a normal distribution for the data likelihood. Then, we let a Markov Chain Monte Carlo algorithm draw samples from the posterior to approximate the posterior for each of the model parameters."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Context for the model\nwith pm.Model() as normal_model:\n    \n    # The prior for the model parameters will be a normal distribution\n    family = pm.glm.families.Normal()\n    \n    # Creating the model requires a formula and data (and optionally a family)\n    pm.GLM.from_formula(formula, data = X_train, family = family)\n    \n    # Perform Markov Chain Monte Carlo sampling\n    normal_trace = pm.sample(draws=2000, chains = 2, tune = 500, njobs=-1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Examine Bayesian Linear Regression Results\nTraceplot of All Samples"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Shows the trace with a vertical line at the mean of the trace\ndef plot_trace(trace):\n    # Traceplot with vertical lines at the mean value\n    ax = pm.traceplot(trace, figsize=(14, len(trace.varnames)*1.8),\n                      lines={k: v['mean'] for k, v in pm.df_summary(trace).iterrows()})\n    \n    matplotlib.rcParams['font.size'] = 16\n    \n    # Labels with the median value\n    for i, mn in enumerate(pm.df_summary(trace)['mean']):\n        ax[i, 0].annotate('{:0.2f}'.format(mn), xy = (mn, 0), xycoords = 'data', size = 8,\n                          xytext = (-18, 18), textcoords = 'offset points', rotation = 90,\n                          va = 'bottom', fontsize = 'large', color = 'red')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plot_trace(normal_trace);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pm.traceplot(normal_trace);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The left side of the traceplot is the marginal posterior: the values for the variable are on the x-axis with the probability for the variable (as determined by sampling) on the y-axis. The different colored lines indicate that we performed two chains of Markov Chain Monte Carlo. From the left side we can see that there is a range of values for each weight. The right side shows the different sample values drawn as the sampling process runs.\nAnother method built into PyMC3 for examinig trace results is the forestplot which shows the distribution of each sampled parameter. This allows us to see the uncertainty in each sample. The forestplot is easily constructed from the trace using pm.forestplot."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pm.forestplot(normal_trace);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "From the forest plot we can see the most likely value of the parameter (the dot) as well as the 95% credible interval for the parameter. The intercept and higher_edu have larger uncertainty compared to the other variables.\nAnother built in plotting method in PyMC3 is the posterior distribution of all the model parameters. These histograms allow us to see how the model result is a distribution for the parameters rather than a single value."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pm.plot_posterior(normal_trace, figsize = (14, 14), text_size=20);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Print out the mean variable weight from the trace\nfor variable in normal_trace.varnames:\n    print('Variable: {:15} Mean weight in model: {:.4f}'.format(variable, \n                                                                np.mean(normal_trace[variable])))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Interpretations of Weights\nBased on the sign and location of the weights, we can make the following inferences regarding the features in our dataset:\nPrevious class failures are negatively related to the students final grade\nHigher education ambitions are positively related to the students grade\nThe mother's and father's education levels are positively related to the students final grade\nStudying time per week is positively related to the students final grade\nAbsences are negatively related to the students final grade"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pm.df_summary(normal_trace)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Linear Formula from Bayesian Inference using Mean of Parameters"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_formula = 'Grade = '\nfor variable in normal_trace.varnames:\n    model_formula += ' %0.2f * %s +' % (np.mean(normal_trace[variable]), variable)\n\n' '.join(model_formula.split(' ')[:-1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Evaluate Bayesian Model Using Mean of Model Parameters"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Evalute the MCMC trace and compare to ml models\ndef evaluate_trace(trace, X_train, X_test, y_train, y_test, model_results):\n    \n    # Dictionary of all sampled values for each parameter\n    var_dict = {}\n    for variable in trace.varnames:\n        var_dict[variable] = trace[variable]\n        \n    # Results into a dataframe\n    var_weights = pd.DataFrame(var_dict)\n    \n    # Means for all the weights\n    var_means = var_weights.mean(axis=0)\n    \n    # Create an intercept column\n    X_test['Intercept'] = 1\n    \n    # Align names of the test observations and means\n    names = X_test.columns[1:]\n    X_test = X_test.ix[:, names]\n    var_means = var_means[names]\n    \n    # Calculate estimate for each test observation using the average weights\n    results = pd.DataFrame(index = X_test.index, columns = ['estimate'])\n\n    for row in X_test.iterrows():\n        results.ix[row[0], 'estimate'] = np.dot(np.array(var_means), np.array(row[1]))\n        \n    # Metrics \n    actual = np.array(y_test)\n    errors = results['estimate'] - actual\n    mae = np.mean(abs(errors))\n    rmse = np.sqrt(np.mean(errors ** 2))\n    \n    print('Model  MAE: {:.4f}\\nModel RMSE: {:.4f}'.format(mae, rmse))\n    \n    # Add the results to the comparison dataframe\n    model_results.ix['Bayesian LR', :] = [mae, rmse]\n    \n    plt.figure(figsize=(12, 8))\n    \n    # Plot median absolute percentage error of all models\n    ax = plt.subplot(1, 2, 1)\n    model_results.sort_values('mae', ascending = True).plot.bar(y = 'mae', color = 'r', ax = ax)\n    plt.title('Model Mean Absolute Error Comparison'); plt.ylabel('MAE'); \n    plt.tight_layout()\n    \n    # Plot root mean squared error of all models\n    ax = plt.subplot(1, 2, 2)\n    model_results.sort_values('rmse', ascending = True).plot.bar(y = 'rmse', color = 'b', ax = ax)\n    plt.title('Model RMSE Comparison'); plt.ylabel('RMSE')\n    \n    return model_results",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "all_model_results = evaluate_trace(normal_trace, X_train, X_test, y_train, y_test, results)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "all_model_results",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Make Predictions from Model\nTest Observations"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Make a new prediction from the test set and compare to actual value\ndef test_model(trace, test_observation):\n    \n    # Print out the test observation data\n    print('Test Observation:')\n    print(test_observation)\n    var_dict = {}\n    for variable in trace.varnames:\n        var_dict[variable] = trace[variable]\n\n    # Results into a dataframe\n    var_weights = pd.DataFrame(var_dict)\n    \n    # Standard deviation of the likelihood\n    sd_value = var_weights['sd'].mean()\n\n    # Actual Value\n    actual = test_observation['Grade']\n    \n    # Add in intercept term\n    test_observation['Intercept'] = 1\n    test_observation = test_observation.drop('Grade')\n    \n    # Align weights and test observation\n    var_weights = var_weights[test_observation.index]\n\n    # Means for all the weights\n    var_means = var_weights.mean(axis=0)\n\n    # Location of mean for observation\n    mean_loc = np.dot(var_means, test_observation)\n    \n    # Estimates of grade\n    estimates = np.random.normal(loc = mean_loc, scale = sd_value,\n                                 size = 1000)\n\n    # Plot all the estimates\n    plt.figure(figsize(8, 8))\n    sns.distplot(estimates, hist = True, kde = True, bins = 19,\n                 hist_kws = {'edgecolor': 'k', 'color': 'darkblue'},\n                kde_kws = {'linewidth' : 4},\n                label = 'Estimated Dist.')\n    # Plot the actual grade\n    plt.vlines(x = actual, ymin = 0, ymax = 5, \n               linestyles = '--', colors = 'red',\n               label = 'True Grade',\n              linewidth = 2.5)\n    \n    # Plot the mean estimate\n    plt.vlines(x = mean_loc, ymin = 0, ymax = 5, \n               linestyles = '-', colors = 'orange',\n               label = 'Mean Estimate',\n              linewidth = 2.5)\n    \n    plt.legend(loc = 1)\n    plt.title('Density Plot for Test Observation');\n    plt.xlabel('Grade'); plt.ylabel('Density');\n    \n    # Prediction information\n    print('True Grade = %d' % actual)\n    print('Average Estimate = %0.4f' % mean_loc)\n    print('5%% Estimate = %0.4f    95%% Estimate = %0.4f' % (np.percentile(estimates, 5),\n                                       np.percentile(estimates, 95)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_model(normal_trace, X_test.iloc[41])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_model(normal_trace, X_test.iloc[16])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Predictions for New Observation"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Make predictions for a new data point from the model trace\ndef query_model(trace, new_observation):\n    \n    # Print information about the new observation\n    print('New Observation')\n    print(new_observation)\n    # Dictionary of all sampled values for each parameter\n    var_dict = {}\n    for variable in trace.varnames:\n        var_dict[variable] = trace[variable]\n        \n    # Standard deviation\n    sd_value = var_dict['sd'].mean()\n    \n    # Results into a dataframe\n    var_weights = pd.DataFrame(var_dict)\n    \n    # Align weights and new observation\n    var_weights = var_weights[new_observation.index]\n    \n    # Means of variables\n    var_means = var_weights.mean(axis=0)\n    \n    # Mean for observation\n    mean_loc = np.dot(var_means, new_observation)\n    \n    # Distribution of estimates\n    estimates = np.random.normal(loc = mean_loc, scale = sd_value,\n                                 size = 1000)\n    \n    # Plot the estimate distribution\n    plt.figure(figsize(8, 8))\n    sns.distplot(estimates, hist = True, kde = True, bins = 19,\n                 hist_kws = {'edgecolor': 'k', 'color': 'darkblue'},\n                kde_kws = {'linewidth' : 4},\n                label = 'Estimated Dist.')\n    # Plot the mean estimate\n    plt.vlines(x = mean_loc, ymin = 0, ymax = 5, \n               linestyles = '-', colors = 'orange', linewidth = 2.5)\n    plt.title('Density Plot for New Observation');\n    plt.xlabel('Grade'); plt.ylabel('Density');\n    \n    # Estimate information\n    print('Average Estimate = %0.4f' % mean_loc)\n    print('5%% Estimate = %0.4f    95%% Estimate = %0.4f' % (np.percentile(estimates, 5),\n                                       np.percentile(estimates, 95)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "observation = pd.Series({'Intercept': 1, 'mother_edu': 4, 'failures': 0, \n                            'higher_edu': 1, 'studytime': 3,\n                            'father_edu': 1, 'absences': 1})\nquery_model(normal_trace, observation)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "observation = pd.Series({'Intercept': 1, 'mother_edu': 2, 'failures': 2, \n                            'higher_edu': 1, 'studytime': 2,\n                            'father_edu': 3, 'absences': 4})\nquery_model(normal_trace, observation)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Model Variable Effects\nIf we want to see the effect of changing one variable while holding the others constant, we can use the function pm.plot_posterior_predictive_glm. This takes a range of values to use for the variable, a linear model, and a number of samples. The function evaluates the linear model across the range of values for the number of samples. Each time, it draws a different set of parameters from the trace. This gives us an indication of the effect of a single variable and also the uncertainty in the model estimates. To see the effect of a single variable, we hold the others constant at their median values."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Examines the effect of changing a single variable\n# Takes in the name of the variable, the trace, and the data\ndef model_effect(query_var, trace, X):\n    \n    # Variables that do not change\n    steady_vars = list(X.columns)\n    steady_vars.remove(query_var)\n    \n    # Linear Model that estimates a grade based on the value of the query variable \n    # and one sample from the trace\n    def lm(value, sample):\n        \n        # Prediction is the estimate given a value of the query variable\n        prediction = sample['Intercept'] + sample[query_var] * value\n        \n        # Each non-query variable is assumed to be at the median value\n        for var in steady_vars:\n            \n            # Multiply the weight by the median value of the variable\n            prediction += sample[var] * X[var].median()\n        \n        return prediction\n    \n    figsize(6, 6)\n    \n    # Find the minimum and maximum values for the range of the query var\n    var_min = X[query_var].min()\n    var_max = X[query_var].max()\n    \n    # Plot the estimated grade versus the range of query variable\n    pm.plot_posterior_predictive_glm(trace, eval=np.linspace(var_min, var_max, 100), \n                                     lm=lm, samples=100, color='blue', \n                                     alpha = 0.4, lw = 2)\n    \n    # Plot formatting\n    plt.xlabel('%s' % query_var, size = 16)\n    plt.ylabel('Grade', size = 16)\n    plt.title(\"Posterior of Grade vs %s\" % query_var, size = 18)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_effect('mother_edu', normal_trace, X_train.drop(columns='Grade'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_effect('studytime', normal_trace, X_train.drop(columns='Grade'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_effect('absences', normal_trace, X_train.drop(columns='Grade'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_effect('failures', normal_trace, X_train.drop(columns='Grade'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_effect('father_edu', normal_trace, X_train.drop(columns='Grade'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Different Likelihood Prior Distribution\nWe can perform the exact same Bayesian Linear Modeling using a Student's T-distribution as the prior for the data likelihood. A Student's T Distribution has more weight in the tails of the distribution so it is more robust to outliers."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# X values for plotting\nx = np.linspace(-5, 5, num = 500)\n\n# Generate pdf of normal distribution\ny_norm = scipy.stats.norm.pdf(x)\n\n# PDF of t-distribution with 2 degrees of freedom\ny_t = scipy.stats.t.pdf(x, df = 2)\n\nplt.plot(x, y_norm, 'b-', label = 'Normal')\nplt.plot(x, y_t, 'r-', label = 'T with 2 df')\nplt.legend(prop = {'size': 18}, loc = 1)\nplt.xlabel('x'); plt.ylabel('Probability'); plt.title('Normal vs T Distribution');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Build Model and Perform Inference"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Context for model\nwith pm.Model() as t_model:\n    # Family is Student's T in this case\n    family = pm.glm.families.StudentT(df = 2)\n    \n    # Formula, data, family\n    pm.GLM.from_formula(formula, data = X_train, family = family)\n    \n    # Sample from the posterior \n    t_trace = pm.sample(draws=2000, tune=500, njobs = -1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plot_trace(t_trace);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Test the Model"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def test_model_t(trace, test_observation):\n    \n    var_dict = {}\n    for variable in trace.varnames:\n        var_dict[variable] = trace[variable]\n\n    # Results into a dataframe\n    var_weights = pd.DataFrame(var_dict)\n\n    # Actual Value\n    actual = test_observation['Grade']\n    \n    # Add in intercept term\n    test_observation['Intercept'] = 1\n    test_observation = test_observation.drop('Grade')\n    \n    # Align weights and test observation\n    var_weights = var_weights[test_observation.index]\n\n    # Means for all the weights\n    var_means = var_weights.mean(axis=0)\n\n    # Location of mean for observation\n    mean_loc = np.dot(var_means, test_observation)\n    \n    # Estimates of grade\n    estimates = mean_loc + np.random.standard_t(df = 2, size = 1000)\n\n    plt.figure(figsize(8, 8))\n    sns.distplot(estimates, hist = True, kde = True, bins = 19,\n                 hist_kws = {'edgecolor': 'k', 'color': 'darkblue'},\n                kde_kws = {'linewidth' : 4},\n                label = 'Estimated Dist.')\n    plt.vlines(x = actual, ymin = 0, ymax = 5, \n               linestyles = '--', colors = 'red',\n               label = 'True Grade',\n              linewidth = 2.5)\n    plt.vlines(x = mean_loc, ymin = 0, ymax = 5, \n               linestyles = '-', colors = 'orange',\n               label = 'Mean Estimate',\n              linewidth = 2.5)\n    \n    plt.legend(loc = 1)\n    plt.title('Density Plot for Test Observation');\n    plt.xlabel('Grade'); plt.ylabel('Density');\n    \n    print('True Grade = %d' % actual)\n    print('Average Estimate = %0.4f' % mean_loc)\n    print('5%% Estimate = %0.4f    95%% Estimate = %0.4f' % (np.percentile(estimates, 5),\n                                       np.percentile(estimates, 95)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_model_t(t_trace, X_test.iloc[60])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_model(normal_trace, X_test.iloc[60])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "New Observation Predictions"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def query_model_t(trace, new_observation):\n    \n    # Dictionary of all sampled values for each parameter\n    var_dict = {}\n    for variable in trace.varnames:\n        var_dict[variable] = trace[variable]\n        \n\n    \n    # Results into a dataframe\n    var_weights = pd.DataFrame(var_dict)\n    \n    # Align weights and new observation\n    var_weights = var_weights[new_observation.index]\n    # Means of variables\n    var_means = var_weights.mean(axis=0)\n    \n    # Mean for observation\n    mean_loc = np.dot(var_means, new_observation)\n    \n    # Distribution of estimates\n    estimates = mean_loc +  np.random.standard_t(df = 2, size = 1000)\n    \n\n    plt.figure(figsize(8, 8))\n    sns.distplot(estimates, hist = True, kde = True, bins = 19,\n                 hist_kws = {'edgecolor': 'k', 'color': 'darkblue'},\n                kde_kws = {'linewidth' : 4},\n                label = 'Estimated Dist.')\n    plt.vlines(x = mean_loc, ymin = 0, ymax = 5, \n               linestyles = '-', colors = 'orange', linewidth = 2.5)\n    plt.title('Density Plot for New Observation');\n    plt.xlabel('Grade'); plt.ylabel('Density');\n    \n    print('Average Estimate = %0.4f' % mean_loc)\n    print('5%% Estimate = %0.4f    95%% Estimate = %0.4f' % (np.percentile(estimates, 5),\n                                       np.percentile(estimates, 95)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "observation = pd.Series({'Intercept': 1, 'mother_edu': 4, 'failures': 0, \n                            'higher_edu': 1, 'studytime': 3,\n                            'father_edu': 1, 'absences': 1})\nquery_model(normal_trace, observation)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "observation = pd.Series({'Intercept': 1, 'mother_edu': 4, 'failures': 0, \n                            'higher_edu': 1, 'studytime': 3,\n                            'father_edu': 1, 'absences': 1})\nquery_model_t(t_trace, observation)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}